projects:
  deepfool:
    name: DeepFool
    category: Learning
    description: Simple algorithm to find the minimum adversarial perturbations in deep networks
    layman_desc: >
      DeepFool is a simple algorithm to find the minimum perturbations
      needed in deep networks to change the outcome of its decision.
    tech_desc: >
      State-of-the-art deep neural networks have achieved impressive results on
      many image classification tasks. However, these same architectures have
      been shown to be unstable to small, well sought, perturbations of the
      images. Despite the importance of this phenomenon, no effective methods
      have been proposed to accurately compute the robustness of
      state-of-the-art deep classifiers to such perturbations on large-scale
      datasets. DeepFool proposes efficiently compute perturbations that fool
      deep networks, and thus reliably quantify the robustness of these
      classifiers.
    code:
      type: Lab GitHub
      url: https://github.com/LTS4/DeepFool
      date_last_commit: 2018-09-07
    contacts:
      - name: Seyed Moosavi
        email: seyed.moosavi@alumni.epfl.ch
    tags:
      - Machine Learning
      - Deep Networks
      - Adversarial
    language: MATLAB, Python
    type: Application
    information:
      - type: Paper
        title: "DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks"
        url: https://infoscience.epfl.ch/record/218057
        notes:
          - label: Published at
            text: 2016 IEEE Conference on Computer Vision and Pattern Recognition
            url: https://cvpr2016.thecvf.com/program/main_conference
    date_added: 2019-03-18
    date_updated: 2021-04-26

  manifool:
    name: ManiFool
    category: Learning
    description: Algorithm for evaluating the invariance properties of deep networks
    tech_desc: >
      Deep convolutional neural networks have been shown to be vulnerable to
      arbitrary geometric transformations. However, there is no systematic
      method to measure the invariance properties of deep networks to such
      transformations. ManiFool is a simple yet scalable algorithm to measure
      the invariance of deep networks. In particular, it measures the
      robustness of deep networks to geometric transformations in a worst-case
      regime as they can be problematic for sensitive applications.
    code:
      type: Personal GitHub
      url: https://github.com/moosavism/ManiFool
      date_last_commit: 2018-01-24
    contacts:
      - name: Seyed Moosavi
        email: seyed.moosavi@alumni.epfl.ch
    tags:
      - Machine Learning
      - Deep Networks
    language: Python
    type: Application
    information:
      - type: Paper
        title: "Geometric Robustness of Deep Networks: Analysis and Improvement"
        url: https://infoscience.epfl.ch/record/253668
        notes:
          - label: Published in
            text: proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition
            url: https://www.computer.org/csdl/proceedings/cvpr/2018/17D45VtKirt
    date_added: 2019-03-18
    date_updated: 2021-04-26

  sparsefool:
    name: SparseFool
    category: Learning
    description: Geometry-inspired sparse attack on deep networks
    layman_desc: >
      Deep Neural Networks have achieved extraordinary results on image
      classification tasks, but have been shown to be vulnerable to attacks
      with carefully crafted perturbations of the input data. Although most
      attacks usually change values of many imageâ€™s pixels, it has been
      shown that deep networks are also vulnerable to sparse alterations
      of the input.
      SparseFool implements an efficient algorithm to compute and control
      sparse alterations.
    code:
      type: Lab GitHub
      url: https://github.com/LTS4/SparseFool
      date_last_commit: 2020-09-27
    contacts:
      - name: Apostolos Modas
        email: apostolos.modas@epfl.ch
    tags:
      - Machine Learning
      - Deep Networks
    language: Python
    license: Apache-2.0
    type: Application
    information:
      - type: Paper
        title: "SparseFool: a few pixels make a big difference"
        url: https://infoscience.epfl.ch/record/258118
        notes:
          - label: Published at
            text: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition
            url: https://cvpr2019.thecvf.com/program/main_conference
    date_added: 2019-09-04
    date_updated: 2021-04-26

  universal:
    name: Universal
    category: Learning
    description: Universal adversarial perturbations
    layman_desc: >
      Proposing a universal (image-agnostic) and very small perturbation
      vector that causes natural images to be misclassified with high
      probability in a deep neuronal network.
    url: https://arxiv.org/pdf/1610.08401.pdf
    code:
      type: Lab GitHub
      url: https://github.com/LTS4/universal
      date_last_commit: 2017-10-23
    contacts:
      - name: Seyed Moosavi
        email: seyed.moosavi@alumni.epfl.ch
    tags:
      - Machine Learning
      - Deep Networks
      - Perturbations
    language: MATLAB, Python
    type: Application
    information:
      - type: Paper
        title: "Universal adversarial perturbations"
        url: https://infoscience.epfl.ch/record/226156
        notes:
          - label: Published in
            text: proceedings of 2017 IEEE Conference on Computer Vision and Pattern Recognition
            url: https://www.computer.org/csdl/proceedings/cvpr/2017/12OmNyoiYVr
    date_added: 2019-03-18
    date_updated: 2021-04-26

  hold-me-tight:
    name: Hold me tight!
    category: Learning
    description: >
      Influence of discriminative features on deep network boundaries in ML
    tech_desc: >
      Important insights towards the explainability of neural networks reside in the characteristics of their
      decision boundaries. In this work, we borrow tools from the field of adversarial robustness, and propose a
      new perspective that relates dataset features to the distance of samples to the decision boundary. This
      enables us to carefully tweak the position of the training samples and measure the induced changes on the
      boundaries of CNNs trained on large-scale vision datasets. We use this framework to reveal some intriguing
      properties of CNNs. Specifically, we rigorously confirm that neural networks exhibit a high invariance to
      non-discriminative features, and show that the decision boundaries of a DNN can only exist as long as the
      classifier is trained with some features that hold them together. Finally, we show that the construction of
      the decision boundary is extremely sensitive to small perturbations of the training samples, and that
      changes in certain directions can lead to sudden invariances in the orthogonal ones. This is precisely the
      mechanism that adversarial training uses to achieve robustness.
    contacts:
      - name: Apostolos Modas
        email: apostolos.modas@epfl.ch
    tags:
      - Deep learning
      - Neural Networks
      - Inductive bias
      - Features
    type: Application
    code:
      type: Lab Github
      url: https://github.com/LTS4/hold-me-tight
      date_last_commit: 2020-10-12
    language: Python
    license: Apache-2.0
    information:
      - type: Paper
        title: "Hold me tight! Influence of discriminative features on deep network boundaries"
        url: https://infoscience.epfl.ch/record/280278
        notes:
          - label: Published at
            text: Neural Information Processing Systems (NeurIPS) 2020
            url: https://neurips.cc/virtual/2020/public/papers.html?filter=sessions&search=poster+session+2
    date_added: 2021-01-27
    date_updated: 2021-04-26

  neural-anisotropy-directions:
    name: Neural Anisotropy Directions
    category: Learning
    description: >
      Analyzing the role of the network architecture in shaping the inductive bias of deep classifiers.
    tech_desc: >
      In this work, we analyze the role of the network architecture in shaping the inductive bias of deep
      classifiers. To that end, we start by focusing on a very simple problem, i.e., classifying a class of
      linearly separable distributions, and show that, depending on the direction of the discriminative feature of
      the distribution, many state-of-the-art deep convolutional neural networks (CNNs) have a surprisingly hard
      time solving this simple task. We then define as neural anisotropy directions (NADs) the vectors that
      encapsulate the directional inductive bias of an architecture. These vectors, which are specific for each
      architecture and hence act as a signature, encode the preference of a network to separate the input data
      based on some particular features. We provide an efficient method to identify NADs for several CNN
      architectures and thus reveal their directional inductive biases. Furthermore, we show that, for the
      CIFAR-10 dataset, NADs characterize the features used by CNNs to discriminate between different classes.
    contacts:
      - name: Apostolos Modas
        email: apostolos.modas@epfl.ch
    tags:
      - Deep learning
      - Neural Networks
      - Inductive bias
      - Features
    type: Application
    code:
      type: Lab Github
      url: https://github.com/LTS4/neural-anisotropy-directions
      date_last_commit: 2020-11-17
    language: Python
    license: Apache-2.0
    information:
      - type: Paper
        title: "Neural Anisotropy Directions"
        url: https://infoscience.epfl.ch/record/280279
        notes:
          - label: Published at
            text: Neural Information Processing Systems (NeurIPS) 2020
            url: https://neurips.cc/virtual/2020/public/papers.html?filter=sessions&search=Poster+Session+4
    date_added: 2021-01-27
    date_updated: 2021-04-26
